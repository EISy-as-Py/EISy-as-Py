{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EISDataImport():\n",
    "    \"\"\"Data Import and Pre-Processing\"\"\"\n",
    "\n",
    "    def DataImporter_Training(self, k, path_List_training,\n",
    "                              image_width, image_height):\n",
    "        \"\"\"\n",
    "        Import the training image file (.png) into the model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        k: The total number of the type.\n",
    "           (Setting the maximum value equal 7 by defult)\n",
    "        path_list_training: A list containing the path of training folder.\n",
    "                            One index for one path only.\n",
    "                            Last index is the nparray file name (XXX.npy).\n",
    "        image_width: The target width after resize\n",
    "        image_height: The target height after resize\n",
    "        \"\"\"\n",
    "        path_list = path_List_training\n",
    "        countImage_Training = [0, 0, 0, 0, 0, 0, 0]\n",
    "        training_data = []\n",
    "        # Iterate the directory\n",
    "        print(len(path_list))\n",
    "        for label in range(len(path_list)):\n",
    "            print(path_list[label])\n",
    "            # Iterate all the image within the directory, f -> the file name\n",
    "            for f in tqdm(os.listdir(path_list[label])):\n",
    "                # Get the full path to the image\n",
    "                path = os.path.join(path_list[label], f)\n",
    "                if \"png\" in path:\n",
    "                    # Read images in the given path and turn into nparray.\n",
    "                    # Convert the iimage to gray scale (optional)\n",
    "                    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (image_width, image_height))\n",
    "                    # Label the image with np.eye() matrix.\n",
    "                    training_data.append([path, np.array(img),\n",
    "                                          np.eye(k-1)[label]])\n",
    "                    for i in range(k):\n",
    "                        if label == i:\n",
    "                            countImage_Training[i] += 1\n",
    "\n",
    "        np.random.shuffle(training_data)\n",
    "        np.save(path_list[-1], training_data)\n",
    "        for i in range(len(path_list)-1):\n",
    "            print(path_List_training[i], \":\", countImage_Training[i])\n",
    "        return training_data\n",
    "\n",
    "    def DataImporter_Predict(self, k, path_List_predict,\n",
    "                             image_width, image_height):\n",
    "        \"\"\"\n",
    "        Import the testing image file (.png) into the model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        k: The total number of path(folder) \n",
    "           (Setting the maximum value equal 10 by defult)\n",
    "        path_list_training: A list containing the path of training folder.\n",
    "                            One index for one path only.\n",
    "                            Last index is the nparray file name (XXX.npy).\n",
    "        image_width: The target width after resize\n",
    "        image_height: The target height after resize\n",
    "        \"\"\"\n",
    "        path_list = path_List_predict\n",
    "        countImage_Predict = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        #training_data = []\n",
    "        # Iterate the directory\n",
    "        for label in range(len(path_list)):\n",
    "            # Iterate all the image within the directory, f -> the file name\n",
    "            for f in tqdm(os.listdir(path_list[label])):\n",
    "                # Get the full path to the image\n",
    "                path = os.path.join(path_list[label], f)\n",
    "                if \"png\" in path:\n",
    "                    # Read images in the given path and turn into nparray.\n",
    "                    # Convert the iimage to gray scale (optional)\n",
    "                    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (image_width, image_height))\n",
    "                    training_data.append([path, np.array(img)])\n",
    "                    # Count the number of image\n",
    "                    for i in range(k):\n",
    "                        if label == i:\n",
    "                            countImage_Predict[i] += 1\n",
    "\n",
    "        np.random.shuffle(training_data)\n",
    "        np.save(path_list[-1], training_data)\n",
    "        for i in range(len(path_list)):\n",
    "            print(path_List_predict[i], \":\", countImage_Predict[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Build_Data(Training, k, path_list, image_width, image_height):\n",
    "    '''This will allow us to use EISData() to preprocess train/test data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Training = True (Class.DataImporter_Training) will be used/ False (vice versa)\n",
    "    k = the total number of path\n",
    "    path_list = [string of folder path]\n",
    "    image_width/height for rescaling\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    training data [path,numpy array for images, label(if Training=True)]\n",
    "    '''\n",
    "    Class = EISData()\n",
    "    if Training is True:\n",
    "        Class.DataImporter_Training(k, path_list, image_width, image_height)\n",
    "    else :\n",
    "        Class.DataImporter_Predict(k, path_list, image_width, image_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1285 [00:00<00:28, 44.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1285/1285 [00:18<00:00, 69.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy : 1285\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NS = \"/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\" # Determine the number of type and then give the directory of each type of image\n",
    "SH = \"/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/SingleHump\"\n",
    "#TH = \"/Neural_Network/TwoHumps\"\n",
    "#TL = \"/Neural_Network/Tail\"\n",
    "LABELS = {NS:0, SH:1}\n",
    "Training = True\n",
    "\n",
    "#print(os.getcwd())\n",
    "path = [NS, SH]\n",
    "print(len(path))\n",
    "k = 2 #two inputs\n",
    "Build_Data=Build_Data(Training, k, path, 800, 536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_training_data(np_ndarray_file):\n",
    "    \"\"\"\n",
    "    Load the data from the .npy file to check if all the images\n",
    "    have been in the program.\n",
    "\n",
    "    Parameter\n",
    "    ----------\n",
    "    np_ndarray_file: The XXX.npy file name.\n",
    "                     Should be identical to the last index in path list\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    training_data:  the dataset expressed in numpy array form.\n",
    "                    type -> numpy.ndarray\n",
    "\n",
    "    \"\"\"\n",
    "    training_data = np.load(np_ndarray_file, allow_pickle=True)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0240_sim_spread-True.png'\n",
      "  array([[255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       ...,\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n",
      "  array([1., 0.])]\n",
      " ['/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0427_sim_spread-True.png'\n",
      "  array([[255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       ...,\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n",
      "  array([1., 0.])]\n",
      " ['/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0007_sim_one-True.png'\n",
      "  array([[255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       ...,\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n",
      "  array([1., 0.])]\n",
      " ...\n",
      " ['/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0755_sim_spread-True.png'\n",
      "  array([[255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       ...,\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n",
      "  array([1., 0.])]\n",
      " ['/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0011_randles_simp-True.png'\n",
      "  array([[255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       ...,\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n",
      "  array([1., 0.])]\n",
      " ['/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0345_sim_spread-True.png'\n",
      "  array([[255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       ...,\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n",
      "  array([1., 0.])]]\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0007_sim_one-True.png',\n",
       "       array([[255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       ...,\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       "       array([1., 0.])], dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data=(load_training_data('SingleHump.npy'))\n",
    "print(training_data)\n",
    "print(len('SingleHump.npy'))\n",
    "training_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_information(training_data):\n",
    "    \"\"\"\n",
    "    Check the size of image and dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_data: the data loading from \"eis_training_data.npy\"\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Type of training_data:\", type(training_data))\n",
    "    print(\"Size of training_data:\", len(training_data))\n",
    "    print(\"Size of image(after rescale):\", training_data[0][1].shape[1],\n",
    "          \"x\", training_data[0][1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of training_data: <class 'numpy.ndarray'>\n",
      "Size of training_data: 1285\n",
      "Size of image(after rescale): 800 x 536\n",
      "(1285, 3)\n",
      "['/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0240_sim_spread-True.png'\n",
      " '/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0427_sim_spread-True.png'\n",
      " '/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0007_sim_one-True.png'\n",
      " ...\n",
      " '/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0755_sim_spread-True.png'\n",
      " '/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0011_randles_simp-True.png'\n",
      " '/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0345_sim_spread-True.png']\n"
     ]
    }
   ],
   "source": [
    "data_information(training_data)\n",
    "print(training_data.shape)\n",
    "print(training_data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ploting_data(training_data, k):\n",
    "    \"\"\"\n",
    "    Show the assigned image with matplotlib package.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_data: the data loading from \"eis_training_data.npy\"\n",
    "    k:  assign one image in training_data to show.\n",
    "        k should fall in the range of dataset size.\n",
    "        Rang: 0-size of training data\n",
    "\n",
    "    \"\"\"\n",
    "    print(training_data[k][0])\n",
    "    plt.imshow(training_data[k][1])\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0016_sim_spread-True.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAD8CAYAAAC8TPVwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ploting_data(training_data, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Convolutional Neural Network Model\"\"\"\n",
    "    def __init__(self, input_size, image_width, image_height,\n",
    "                 firstHidden, kernel_size, output_size):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size:\n",
    "        image_width: The width of input images.\n",
    "                     this is provided from the data_information function\n",
    "        image_height: The width of input images.\n",
    "                     this is provided from the data_information function\n",
    "        firstHidden: The size of first hidden layer.\n",
    "                     The size of next layer will be twice of the current layer\n",
    "                     Ex: 1st is 8, 2nd will be 16, 3rd will be 24 and so on.\n",
    "                     Number of hidden layer is set as 4 by default.\n",
    "        kernel_size: It will form a subwindom with size of kernel to scan over\n",
    "                     the original image.\n",
    "                     kernel_size must be an odd integer,\n",
    "                     usually not larger than 7\n",
    "        output_size: The number of final target category.\n",
    "\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, firstHidden, kernel_size)\n",
    "        self.conv2 = nn.Conv2d(firstHidden, firstHidden*2, kernel_size)\n",
    "        self.conv3 = nn.Conv2d(firstHidden*2, firstHidden*4, kernel_size)\n",
    "        self.conv4 = nn.Conv2d(firstHidden*4, firstHidden*8, kernel_size)\n",
    "        # Get size\n",
    "        x = torch.randn(image_height, image_width).view(-1, 1, image_height,\n",
    "                                                        image_width)\n",
    "        conv_to_linear = self.last_conv_neuron(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(conv_to_linear, 64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "\n",
    "    def last_conv_neuron(self, x):\n",
    "        \"\"\"\n",
    "        Calculate how many neurons that the last convolutional layer will\n",
    "        connect to the linear hidden layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: a random torch tensor with size (-1, 1, image_height, image_width)\n",
    "        Ex: x = torch.randn(image_height, image_width\n",
    "                            ).view(-1, 1, image_height, image_width)\n",
    "        \"\"\"\n",
    "        x = self.convs(x)\n",
    "        conv_to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return conv_to_linear\n",
    "\n",
    "    def convs(self, x):\n",
    "        \"\"\"\n",
    "        Put the image into the convolutional hidden layer. Scan over the\n",
    "        original image to and use the max pooling function (with size 2) to\n",
    "        determine the one number to represent the sub-image.\n",
    "\n",
    "        \"\"\"\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv4(x)), (2, 2))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.convs(x)\n",
    "        conv_to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        # Flatten the data\n",
    "        xF = x.view(-1, conv_to_linear)\n",
    "        # put into the first fully connected layer\n",
    "        output = torch.sigmoid(self.fc1(xF))\n",
    "        output = self.fc2(output)\n",
    "        return F.softmax(output, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=95232, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(1,800, 536, 8, 3, 2) #(input_size, W,H,first hidden, kernel size,output size)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_to_tensor(training_data, image_height, image_width):\n",
    "    \"\"\"Transform the array image into tensor.\"\"\"\n",
    "    X = torch.Tensor([i[1] for i in training_data]\n",
    "                     ).view(-1, image_height, image_width)\n",
    "    return X/255.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=image_to_tensor(training_data, 536, 800)\n",
    "#X=X/255. #normalizing X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def type_to_tensor(training_data):\n",
    "    \"\"\"Transform the array type into tensor.\"\"\"\n",
    "    y = torch.Tensor([i[2] for i in training_data])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=type_to_tensor(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n"
     ]
    }
   ],
   "source": [
    "#X = torch.Tensor([i[0] for i in training_data]).view(-1, 536, 800)\n",
    "#X = X/255.0\n",
    "#y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.2 # 20% for Testing\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample: 1028\n",
      "Testing Sample: 257\n"
     ]
    }
   ],
   "source": [
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(\"Training Sample:\", len(train_X))\n",
    "print(\"Testing Sample:\", len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_separation(data, ratio_of_testing, TRAIN):\n",
    "    \"\"\"Separate the training and testing data.\"\"\"\n",
    "    VAL_PCT = ratio_of_testing\n",
    "    val_size = int(len(data)*VAL_PCT)\n",
    "\n",
    "    if TRAIN is True:\n",
    "        train_data = data[:-val_size]\n",
    "        print(\"Training Samples:\", len(train_data))\n",
    "        return train_data\n",
    "    test_data = data[-val_size:]\n",
    "    print(\"Testing Samples:\", len(test_data))\n",
    "    return test_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Samples: 257\n"
     ]
    }
   ],
   "source": [
    "ratio_of_testing=0.2\n",
    "test_data=data_separation(X, ratio_of_testing, train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning(train_data1, train_data2, input_size, image_width, image_height,\n",
    "             firstHidden, kernel_size, output_size, learning_rate, BATCH_SIZE,\n",
    "             EPOCHS):\n",
    "    \"\"\"\n",
    "    parameter\n",
    "    ---------\n",
    "    train_data1 = X_train [image data]\n",
    "    train_data2 = y_train [type data]\n",
    "    input_size = 1\n",
    "    image_width, image_height = same as the ones defined for preprocessing\n",
    "    firstHidden = 8, typically\n",
    "    kernel_size = 4\n",
    "    output_size = 2 [0=bad, 1=pass]\n",
    "    learning_rate = 0.001 at default for pytorch\n",
    "    BATCH_SIZE = 10 the number of images used for training at a time\n",
    "    EPOCHS = 1\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(Net(input_size, image_width, image_height,\n",
    "                               firstHidden, kernel_size, output_size\n",
    "                               ).parameters(), lr=learning_rate) \n",
    "    loss_function = nn.L1Loss() #L1Loss() used for our model (complicated cnn model)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_data1), BATCH_SIZE)):\n",
    "            batch_data1 = train_data1[i:i+BATCH_SIZE].view(-1, 1,\n",
    "                                                           image_height,\n",
    "                                                           image_width)\n",
    "            batch_data2 = train_data2[i:i+BATCH_SIZE]\n",
    "\n",
    "            Net(input_size, image_width, image_height, firstHidden,\n",
    "                kernel_size, output_size).zero_grad()\n",
    "            outputs = Net(input_size, image_width, image_height, firstHidden,\n",
    "                          kernel_size, output_size)(batch_data1)\n",
    "            loss = loss_function(outputs, batch_data2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [08:32<00:00,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5772, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss = learning(train_X, train_y, 1, 800, 536, 8, 4, 2, 0.001, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(test_data1, test_data2, input_size, image_width, image_height,\n",
    "             firstHidden, kernel_size, output_size):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_data1))):\n",
    "            real_type = torch.argmax(test_data2[i])\n",
    "            net_out_train = Net(input_size, image_width, image_height,\n",
    "                                firstHidden, kernel_size, output_size\n",
    "                                )(test_data1[i].view(-1, 1, image_height,\n",
    "                                                     image_width))[0]\n",
    "            predicted_type = torch.argmax(net_out_train)\n",
    "\n",
    "            if predicted_type == real_type:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    print(\"Accuracy:\", round(correct/total, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-308aa71f0958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m accuracy = accuracy(test_X, test_y, 1, 800, 536,\n\u001b[0m\u001b[1;32m      2\u001b[0m              8, 4, 2)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy(test_X, test_y, 1, 800, 536,\n",
    "             8, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def type_prediction(k, path_List_training, tensor_data, array_data,\n",
    "                    input_size, image_width, image_height, firstHidden,\n",
    "                    kernel_size, output_size, detailed_information):\n",
    "    \"\"\"\n",
    "    Predict which type the input image is and print out the total number of\n",
    "    each type.\n",
    "    (Optional) Print out the predicted type and file name for each image.\n",
    "    Parameters\n",
    "    ----------\n",
    "    k\n",
    "    path\n",
    "    Same as the parameters of nn model\n",
    "    input_size\n",
    "    image_width: The target width after resize\n",
    "    image_height: The target height after resize\n",
    "    firstHidden: The size of first hidden layer.\n",
    "    kernel_size: It will form a subwindom with size of kernel to scan over\n",
    "                 the original image.\n",
    "    output_size: The number of final target type.\n",
    "    detailed information: Show the predicted type and file\n",
    "                          name for each image or not\n",
    "    \"\"\"\n",
    "    countImage_predicted_type = [0, 0, 0, 0, 0, 0, 0]\n",
    "    for i in range(len(Input_data)):\n",
    "        net_out_predict = Net(input_size, image_width, image_height,\n",
    "                              firstHidden, kernel_size, output_size\n",
    "                              )(Input_data[i].view(-1, 1, image_height,\n",
    "                                                   image_width))[0]\n",
    "        predicted_type = torch.argmax(net_out_predict)\n",
    "        for Type in range(k):\n",
    "            if predicted_type == 0:\n",
    "                countImage_predicted_type[Type] += 1\n",
    "                # Print out the detailed information.\n",
    "                if detailed_information is True:\n",
    "                    print(\"Warning! Type Prediction:\", path_List_training[Type])\n",
    "                    print(\"Path and File Name\", array_data[i][0])\n",
    "            else:\n",
    "                countImage_predicted_type[Type] += 1\n",
    "                \n",
    "\n",
    "    for i in range(len(path_List_training)-1):\n",
    "        print(path_List_training[i], \":\", countImage_predicted_type[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0427_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0185_randles_simp-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0200_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0681_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0162_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0492_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0106_sim_one-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0360_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0641_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0135_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0078_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0912_sim_spread-True(1).png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0266_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0592_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0517_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0871_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0002_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0334_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0074_sim_one-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0154_randles_simp-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0391_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0025_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0008_randles_simp-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0306_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0025_randles_simp-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0317_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0092_sim_one-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0024_randles_simp-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0863_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0010_sim_one-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0399_sim_spread-True.png\n",
      "Warning! Type Prediction: /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy\n",
      "Path and File Name /Users/mkim91/Desktop/Direct/Eisy_as_py/eisy/cnn/Neural_Network/Noisy/200315-0168_sim_spread-True.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-b3a8b43a3f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m type_prediction(k, path_List_training, Input_data, np_array_data,\n\u001b[1;32m     13\u001b[0m                     \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirstHidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     kernel_size, output_size,detailed)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-fcce719132d1>\u001b[0m in \u001b[0;36mtype_prediction\u001b[0;34m(k, path_List_training, tensor_data, array_data, input_size, image_width, image_height, firstHidden, kernel_size, output_size, detailed_information)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         net_out_predict = Net(input_size, image_width, image_height,\n\u001b[0;32m---> 26\u001b[0;31m                               \u001b[0mfirstHidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                               )(Input_data[i].view(-1, 1, image_height,\n\u001b[1;32m     28\u001b[0m                                                    image_width))[0]\n",
      "\u001b[0;32m<ipython-input-99-5dd584bf4c14>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, image_width, image_height, firstHidden, kernel_size, output_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mconv_to_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_conv_neuron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_to_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_List_training= [NS, SH]\n",
    "k=len(path_List_training)-1\n",
    "Input_data=X\n",
    "np_array_data=training_data\n",
    "input_size=1\n",
    "image_width=800\n",
    "image_height=536\n",
    "firstHidden=8\n",
    "kernel_size=4\n",
    "output_size=2\n",
    "detailed= True\n",
    "type_prediction(k, path_List_training, Input_data, np_array_data,\n",
    "                    input_size, image_width, image_height, firstHidden,\n",
    "                    kernel_size, output_size,detailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
